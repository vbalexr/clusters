apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: llama-cpp
  labels:
    app: llama-cpp
spec:
  serviceName: llama-cpp-headless
  replicas: 1
  selector:
    matchLabels:
      app: llama-cpp
    
  template:
    metadata:
      labels:
        app: llama-cpp
    spec:
      containers:
        - name: llama-cpp
          image: ghcr.io/ggml-org/llama.cpp:server
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            - name: MODEL_FILENAME
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-model
                  key: MODEL_FILENAME
            - name: MMPROJ_FILENAME
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-model
                  key: MMPROJ_FILENAME
            - name: CTX_SIZE
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-model
                  key: CTX_SIZE
            - name: N_GPU_LAYERS
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-model
                  key: N_GPU_LAYERS
            - name: TEMPERATURE
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-model
                  key: TEMPERATURE
            - name: TOP_P
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-model
                  key: TOP_P
            - name: TOP_K
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-model
                  key: TOP_K
            - name: REPEAT_PENALTY
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-model
                  key: REPEAT_PENALTY
            - name: BATCH_SIZE
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-model
                  key: BATCH_SIZE
            - name: PARALLEL
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-model
                  key: PARALLEL
          command:
            - /bin/sh
            - -c
            - |
              set -eu
              set -- /app/llama-server \
                --host 0.0.0.0 \
                --port 8080 \
                --model "/models/${MODEL_FILENAME}" \
                --ctx-size "${CTX_SIZE}" \
                --batch-size "${BATCH_SIZE}" \
                --parallel "${PARALLEL}" \
                --n-gpu-layers "${N_GPU_LAYERS}" \
                --temp "${TEMPERATURE}" \
                --top-p "${TOP_P}" \
                --top-k "${TOP_K}" \
                --repeat-penalty "${REPEAT_PENALTY}"

              if [ -n "${MMPROJ_FILENAME:-}" ] && [ -f "/models/${MMPROJ_FILENAME}" ]; then
                set -- "$@" --mmproj "/models/${MMPROJ_FILENAME}"
              fi

              exec "$@"
          volumeMounts:
            - name: models
              mountPath: /models
              readOnly: true
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 20
          resources:
            requests:
              cpu: 500m
              memory: 4Gi
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-cpp-models

