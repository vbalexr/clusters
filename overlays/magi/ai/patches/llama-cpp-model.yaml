apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-model
data:
  MODEL_URL: https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3VL-8B-Instruct-Q4_K_M.gguf?download=true
  MODEL_FILENAME: Qwen3VL-8B-Instruct-Q4_K_M.gguf

  # Runtime tuning
  CTX_SIZE: "4096"

  # Attempt to offload as many layers as possible to the Intel iGPU.
  # llama.cpp will clamp this to the model's layer count.
  N_GPU_LAYERS: "999"
