apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-model
data:
  MODEL_URL: https://huggingface.co/lmstudio-community/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q4_K_M.gguf?download=true
  MODEL_FILENAME: Qwen2.5-14B-Instruct-Q4_K_M.gguf

  # Runtime tuning
  CTX_SIZE: "8192"

  # Attempt to offload as many layers as possible to the Intel iGPU.
  # llama.cpp will clamp this to the model's layer count.
  N_GPU_LAYERS: "999"
