apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-model
data:
  # Qwen2.5-Coder 7B Instruct (GGUF, Q4_K_M). Fits comfortably under 20Gi RAM.
  MODEL_URL: https://huggingface.co/bartowski/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf
  MODEL_FILENAME: Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf

  # Runtime tuning
  CTX_SIZE: "4096"

  # Attempt to offload as many layers as possible to the Intel iGPU.
  # llama.cpp will clamp this to the model's layer count.
  N_GPU_LAYERS: "999"
